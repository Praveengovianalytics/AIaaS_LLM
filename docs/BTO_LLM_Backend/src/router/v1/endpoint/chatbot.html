<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>BTO_LLM_Backend.src.router.v1.endpoint.chatbot API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>BTO_LLM_Backend.src.router.v1.endpoint.chatbot</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os

import cachetools
from fastapi import APIRouter, Header, HTTPException

import shutil

from langchain.chains import ConversationalRetrievalChain

from core.controller.orchestration_layer.model import LLM
from core.schema.api_response import APIResponse
from core.schema.feedback_request import FeedbackRequest
from core.schema.prediction_request import PredictionRequest
from core.settings import Param
from core.controller.orchestration_layer.embedding_pipeline import (
    EmbeddingPipeline,
    load_embedding,
)
from langchain.llms import CTransformers
from fastapi import UploadFile
from core.controller.authentication_layer.jwt import decodeJWT
from core.limiter import limiter
from starlette.requests import Request
from starlette.responses import Response

from core.schema.prediction_request import ModelRequest

## Use In-Memory Ram
user_model_cache = cachetools.LRUCache(maxsize=20)

llm = CTransformers(
    model=Param.LLM_MODEL_PATH,
    model_type=Param.LLM_MODEL_TYPE,
    config={
        &#34;max_new_tokens&#34;: Param.LLM_MAX_NEW_TOKENS,
        &#34;temperature&#34;: Param.LLM_TEMPERATURE,
        &#34;top_k&#34;: Param.TOP_K,
        &#34;top_p&#34;: Param.TOP_P,
        &#34;batch_size&#34;: Param.BATCH_SIZE

    }

)
router = APIRouter()


@router.get(&#34;/get_configure&#34;)
@limiter.limit(&#34;5/second&#34;)
def get_configuration(request: Request, response: Response):
    &#34;&#34;&#34;
    The get_configuration function is a simple function that returns the configuration of the Model.
        ---
        description: Returns the configuration of this API.
        responses:
          200:  # HTTP Status code 200 means &amp;quot;OK&amp;quot; (the request was fulfilled)
            description: The health check passed and this API is healthy!

    Returns:
        A dictionary with a key of &amp;quot;status&amp;quot; and a value of &amp;quot;healthy&amp;quot;
    &#34;&#34;&#34;
    return {&#34;status&#34;: &#34;success&#34;, &#34;config&#34;: {
        &#34;max_new_tokens&#34;: Param.LLM_MAX_NEW_TOKENS,
        &#34;temperature&#34;: Param.LLM_TEMPERATURE,
        &#34;top_k&#34;: Param.TOP_K,
        &#34;top_p&#34;: Param.TOP_P,
        &#34;batch_size&#34;: Param.BATCH_SIZE

    }}


@router.get(&#34;/ping&#34;)
@limiter.limit(&#34;5/second&#34;)
def health_check(request: Request, response: Response):
    &#34;&#34;&#34;
    The health_check function is a simple function that returns the status of the API.
        ---
        description: Returns the health of this API.
        responses:
          200:  # HTTP Status code 200 means &amp;quot;OK&amp;quot; (the request was fulfilled)
            description: The health check passed and this API is healthy!

    Returns:
        A dictionary with a key of &amp;quot;status&amp;quot; and a value of &amp;quot;healthy&amp;quot;
    &#34;&#34;&#34;
    return {&#34;status&#34;: &#34;healthy&#34;}


@router.post(&#34;/set_model&#34;)
@limiter.limit(&#34;5/second&#34;)
def set_model(request: Request, response: Response, data: ModelRequest, authorization: str = Header(None),
              ):
    &#34;&#34;&#34;
    The set_model function is a simple function that initialise the model for user.
        ---
        description: Returns the health of this API.
        responses:
          200:  # HTTP Status code 200 means &amp;quot;OK&amp;quot; (the request was fulfilled)

    Returns:
        A dictionary with a key of &amp;quot;status&amp;quot; and a value of &amp;quot;healthy&amp;quot;
    &#34;&#34;&#34;
    auth = decodeJWT(authorization)
    if auth[&#34;valid&#34;]:
        try:
            if (data.config.items() == {
                &#34;max_new_tokens&#34;: Param.LLM_MAX_NEW_TOKENS,
                &#34;temperature&#34;: Param.LLM_TEMPERATURE,
                &#34;top_k&#34;: Param.TOP_K,
                &#34;top_p&#34;: Param.TOP_P,
                &#34;batch_size&#34;: Param.BATCH_SIZE

            }.items()):
                print(&#39;Default Config&#39;)
            else:
                exist = &#39;&#39;
                for i in user_model_cache.keys():
                    if data.config.items() == user_model_cache[i][&#39;config&#39;].items():
                        exist = i
                        print(&#39;Exist Model in Cache&#39;)

                if exist == &#39;&#39;:
                    custom_llm = CTransformers(
                        model=Param.LLM_MODEL_PATH,
                        model_type=Param.LLM_MODEL_TYPE,
                        config=data.config

                    )
                    user_model_cache[auth[&#34;data&#34;][&#34;username&#34;]] = {&#39;model&#39;: custom_llm, &#39;config&#39;: data.config}
                    print(&#39;Created New Model in Cache&#39;)

            return APIResponse(status=&#34;success&#34;, message=&#39;Model Initialisation Success&#39;)
        except Exception as e:
            print(e)
            return APIResponse(status=&#34;fail&#34;, message=&#39;Model Initialisation Failed&#39;)


@router.post(&#34;/create_embedding&#34;)
@limiter.limit(&#34;5/second&#34;)
def create_embedding(
        request: Request,
        response: Response,
        file: UploadFile,
        authorization: str = Header(None),
):
    &#34;&#34;&#34;
    The create_embedding function creates a new embedding file.

    Args:

        file: File Received
        authorization: str: Get the jwt token from the header
        : Get the embedding model for a particular user

    Returns:
        A success message if the embedding is created successfully&#34;&#34;&#34;
    auth = decodeJWT(authorization)
    if auth[&#34;valid&#34;]:
        user_folder = Param.EMBEDDING_MODEL_PATH + auth[&#34;data&#34;][&#34;username&#34;] + &#34;/&#34;
        if os.path.exists(user_folder):
            shutil.rmtree(user_folder)
        else:
            os.makedirs(user_folder)

        file_location = f&#34;{Param.TEMP_SAVE_PATH}/{file.filename}&#34;

        with open(file_location, &#34;wb+&#34;) as file_object:
            file_object.write(file.file.read())
        embedding = EmbeddingPipeline(file_location, auth[&#34;data&#34;][&#34;username&#34;])
        embedding.save_db_local()
        if os.path.isfile(file_location):
            os.remove(file_location)
        return APIResponse(status=&#34;success&#34;, message=&#34;Embedding Created Success&#34;)
    else:
        return HTTPException(401, detail=&#34;Unauthorised&#34;)


def retrieve_model(data, username):
    print(user_model_cache.keys())
    if data.use_default == 1:
        llms = llm
        return llms
    else:
        if (username in user_model_cache.keys() ):
            print(&#39;Exist Model in Cache&#39;)

            return user_model_cache[username][&#39;model&#39;]
        else:

            for i in user_model_cache.keys():
                if data.config.items() == user_model_cache[i][&#39;config&#39;].items():
                    print(&#39;Exist Model in Cache&#39;)
                    llms = user_model_cache[i][&#39;model&#39;]
                    return llms

            custom_llm = CTransformers(
                model=Param.LLM_MODEL_PATH,
                model_type=Param.LLM_MODEL_TYPE,
                config=data.config

            )
            user_model_cache[username] = {&#39;model&#39;: custom_llm, &#39;config&#39;: data.config}
            print(&#39;Created New Model in Cache&#39;)
            return user_model_cache[username][&#39;model&#39;]


@router.post(&#34;/predict&#34;)
@limiter.limit(&#34;5/second&#34;)
def predict(
        request: Request,
        response: Response,
        data: PredictionRequest,
        authorization: str = Header(None),
):
    &#34;&#34;&#34;
    The predict function is the main function of this API. It takes in a query and chat history,
    and returns a response from the model. The predict function also requires an authorization header
    which contains a JWT token that has been signed by our server.

    Args:
        data: PredictionRequest: Pass the query and chat history to the predict function
        authorization: str: Pass the jwt token to the function
        : Pass the query and chat history to the model

    Returns:
        A predictions response&#34;&#34;&#34;
    auth = decodeJWT(authorization)
    if auth[&#34;valid&#34;]:
        retriever = load_embedding(
            Param.EMBEDDING_SAVE_PATH + auth[&#34;data&#34;][&#34;username&#34;] + &#34;/&#34;
        )
        llms = retrieve_model(data, auth[&#34;data&#34;][&#34;username&#34;])
        chain = ConversationalRetrievalChain.from_llm(
            llm=llms, retriever=retriever.as_retriever()
        )
        result = LLM(chain, llms, retriever).predict(data.query, data.chat_history)

        return APIResponse(status=&#34;success&#34;, message=result)
    else:
        return HTTPException(401, detail=&#34;Unauthorised&#34;)


@router.post(&#34;/feedback&#34;)
@limiter.limit(&#34;5/second&#34;)
def feedback(
        request: Request,
        response: Response,
        data: FeedbackRequest,
        authorization: str = Header(None),
):
    &#34;&#34;&#34;
    The feedback function is used to log the user&#39;s feedback on a particular interaction with the bot.
    The function takes in a request object, response object and data from the request body. The data contains
    the chat history of that interaction, timestamps for each message sent by both parties and finally
    the user&#39;s feedback on that interaction.

    Args:
        data: FeedbackRequest: Receive the feedback data from the user
        authorization: str: Pass the jwt token in the header
        : Get the user&#39;s feedback

    Returns:
        A json object with the status and message&#34;&#34;&#34;
    auth = decodeJWT(authorization)
    if auth[&#34;valid&#34;]:
        with open(
                Param.FEEDBACK_LOG_FILE + &#34;feedback_&#34; + auth[&#34;data&#34;][&#34;username&#34;] + &#34;.txt&#34;,
                &#34;a+&#34;,
                encoding=&#34;utf-8&#34;,
        ) as log_file:
            log_file.write(
                f&#34;User_Timestamp: {data.user_timestamp} | User_Input: {data.chat_history[len(data.chat_history) - 2]}\n&#34;
            )
            log_file.write(
                f&#34;Bot_Timestamp: {data.bot_timestamp} | Bot_Response: {data.chat_history[-1]}\n&#34;
            )
            log_file.write(
                f&#34;Feedback_Timestamp: {data.feedback_timestamp} | User_Feedback: {data.feedback}\n&#34;
            )
            log_file.write(&#34;=&#34; * 50 + &#34;\n&#34;)

        return APIResponse(status=&#34;success&#34;, message=&#34;Feedback Noted&#34;)
    else:
        return HTTPException(401, detail=&#34;Unauthorised&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="BTO_LLM_Backend.src.router.v1.endpoint.chatbot.create_embedding"><code class="name flex">
<span>def <span class="ident">create_embedding</span></span>(<span>request: starlette.requests.Request, response: starlette.responses.Response, file: fastapi.datastructures.UploadFile, authorization: str = Header(None))</span>
</code></dt>
<dd>
<div class="desc"><p>The create_embedding function creates a new embedding file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file</code></strong></dt>
<dd>File Received</dd>
<dt><strong><code>authorization</code></strong></dt>
<dd>str: Get the jwt token from the header</dd>
<dd>Get the embedding model for a particular user</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A success message if the embedding is created successfully</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@router.post(&#34;/create_embedding&#34;)
@limiter.limit(&#34;5/second&#34;)
def create_embedding(
        request: Request,
        response: Response,
        file: UploadFile,
        authorization: str = Header(None),
):
    &#34;&#34;&#34;
    The create_embedding function creates a new embedding file.

    Args:

        file: File Received
        authorization: str: Get the jwt token from the header
        : Get the embedding model for a particular user

    Returns:
        A success message if the embedding is created successfully&#34;&#34;&#34;
    auth = decodeJWT(authorization)
    if auth[&#34;valid&#34;]:
        user_folder = Param.EMBEDDING_MODEL_PATH + auth[&#34;data&#34;][&#34;username&#34;] + &#34;/&#34;
        if os.path.exists(user_folder):
            shutil.rmtree(user_folder)
        else:
            os.makedirs(user_folder)

        file_location = f&#34;{Param.TEMP_SAVE_PATH}/{file.filename}&#34;

        with open(file_location, &#34;wb+&#34;) as file_object:
            file_object.write(file.file.read())
        embedding = EmbeddingPipeline(file_location, auth[&#34;data&#34;][&#34;username&#34;])
        embedding.save_db_local()
        if os.path.isfile(file_location):
            os.remove(file_location)
        return APIResponse(status=&#34;success&#34;, message=&#34;Embedding Created Success&#34;)
    else:
        return HTTPException(401, detail=&#34;Unauthorised&#34;)</code></pre>
</details>
</dd>
<dt id="BTO_LLM_Backend.src.router.v1.endpoint.chatbot.feedback"><code class="name flex">
<span>def <span class="ident">feedback</span></span>(<span>request: starlette.requests.Request, response: starlette.responses.Response, data: core.schema.feedback_request.FeedbackRequest, authorization: str = Header(None))</span>
</code></dt>
<dd>
<div class="desc"><p>The feedback function is used to log the user's feedback on a particular interaction with the bot.
The function takes in a request object, response object and data from the request body. The data contains
the chat history of that interaction, timestamps for each message sent by both parties and finally
the user's feedback on that interaction.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>FeedbackRequest: Receive the feedback data from the user</dd>
<dt><strong><code>authorization</code></strong></dt>
<dd>str: Pass the jwt token in the header</dd>
<dd>Get the user's feedback</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A json object with the status and message</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@router.post(&#34;/feedback&#34;)
@limiter.limit(&#34;5/second&#34;)
def feedback(
        request: Request,
        response: Response,
        data: FeedbackRequest,
        authorization: str = Header(None),
):
    &#34;&#34;&#34;
    The feedback function is used to log the user&#39;s feedback on a particular interaction with the bot.
    The function takes in a request object, response object and data from the request body. The data contains
    the chat history of that interaction, timestamps for each message sent by both parties and finally
    the user&#39;s feedback on that interaction.

    Args:
        data: FeedbackRequest: Receive the feedback data from the user
        authorization: str: Pass the jwt token in the header
        : Get the user&#39;s feedback

    Returns:
        A json object with the status and message&#34;&#34;&#34;
    auth = decodeJWT(authorization)
    if auth[&#34;valid&#34;]:
        with open(
                Param.FEEDBACK_LOG_FILE + &#34;feedback_&#34; + auth[&#34;data&#34;][&#34;username&#34;] + &#34;.txt&#34;,
                &#34;a+&#34;,
                encoding=&#34;utf-8&#34;,
        ) as log_file:
            log_file.write(
                f&#34;User_Timestamp: {data.user_timestamp} | User_Input: {data.chat_history[len(data.chat_history) - 2]}\n&#34;
            )
            log_file.write(
                f&#34;Bot_Timestamp: {data.bot_timestamp} | Bot_Response: {data.chat_history[-1]}\n&#34;
            )
            log_file.write(
                f&#34;Feedback_Timestamp: {data.feedback_timestamp} | User_Feedback: {data.feedback}\n&#34;
            )
            log_file.write(&#34;=&#34; * 50 + &#34;\n&#34;)

        return APIResponse(status=&#34;success&#34;, message=&#34;Feedback Noted&#34;)
    else:
        return HTTPException(401, detail=&#34;Unauthorised&#34;)</code></pre>
</details>
</dd>
<dt id="BTO_LLM_Backend.src.router.v1.endpoint.chatbot.get_configuration"><code class="name flex">
<span>def <span class="ident">get_configuration</span></span>(<span>request: starlette.requests.Request, response: starlette.responses.Response)</span>
</code></dt>
<dd>
<div class="desc"><p>The get_configuration function is a simple function that returns the configuration of the Model.
&mdash;
description: Returns the configuration of this API.
responses:
200:
# HTTP Status code 200 means &quot;OK&quot; (the request was fulfilled)
description: The health check passed and this API is healthy!</p>
<h2 id="returns">Returns</h2>
<p>A dictionary with a key of &quot;status&quot; and a value of &quot;healthy&quot;</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@router.get(&#34;/get_configure&#34;)
@limiter.limit(&#34;5/second&#34;)
def get_configuration(request: Request, response: Response):
    &#34;&#34;&#34;
    The get_configuration function is a simple function that returns the configuration of the Model.
        ---
        description: Returns the configuration of this API.
        responses:
          200:  # HTTP Status code 200 means &amp;quot;OK&amp;quot; (the request was fulfilled)
            description: The health check passed and this API is healthy!

    Returns:
        A dictionary with a key of &amp;quot;status&amp;quot; and a value of &amp;quot;healthy&amp;quot;
    &#34;&#34;&#34;
    return {&#34;status&#34;: &#34;success&#34;, &#34;config&#34;: {
        &#34;max_new_tokens&#34;: Param.LLM_MAX_NEW_TOKENS,
        &#34;temperature&#34;: Param.LLM_TEMPERATURE,
        &#34;top_k&#34;: Param.TOP_K,
        &#34;top_p&#34;: Param.TOP_P,
        &#34;batch_size&#34;: Param.BATCH_SIZE

    }}</code></pre>
</details>
</dd>
<dt id="BTO_LLM_Backend.src.router.v1.endpoint.chatbot.health_check"><code class="name flex">
<span>def <span class="ident">health_check</span></span>(<span>request: starlette.requests.Request, response: starlette.responses.Response)</span>
</code></dt>
<dd>
<div class="desc"><p>The health_check function is a simple function that returns the status of the API.
&mdash;
description: Returns the health of this API.
responses:
200:
# HTTP Status code 200 means &quot;OK&quot; (the request was fulfilled)
description: The health check passed and this API is healthy!</p>
<h2 id="returns">Returns</h2>
<p>A dictionary with a key of &quot;status&quot; and a value of &quot;healthy&quot;</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@router.get(&#34;/ping&#34;)
@limiter.limit(&#34;5/second&#34;)
def health_check(request: Request, response: Response):
    &#34;&#34;&#34;
    The health_check function is a simple function that returns the status of the API.
        ---
        description: Returns the health of this API.
        responses:
          200:  # HTTP Status code 200 means &amp;quot;OK&amp;quot; (the request was fulfilled)
            description: The health check passed and this API is healthy!

    Returns:
        A dictionary with a key of &amp;quot;status&amp;quot; and a value of &amp;quot;healthy&amp;quot;
    &#34;&#34;&#34;
    return {&#34;status&#34;: &#34;healthy&#34;}</code></pre>
</details>
</dd>
<dt id="BTO_LLM_Backend.src.router.v1.endpoint.chatbot.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>request: starlette.requests.Request, response: starlette.responses.Response, data: core.schema.prediction_request.PredictionRequest, authorization: str = Header(None))</span>
</code></dt>
<dd>
<div class="desc"><p>The predict function is the main function of this API. It takes in a query and chat history,
and returns a response from the model. The predict function also requires an authorization header
which contains a JWT token that has been signed by our server.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>PredictionRequest: Pass the query and chat history to the predict function</dd>
<dt><strong><code>authorization</code></strong></dt>
<dd>str: Pass the jwt token to the function</dd>
<dd>Pass the query and chat history to the model</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A predictions response</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@router.post(&#34;/predict&#34;)
@limiter.limit(&#34;5/second&#34;)
def predict(
        request: Request,
        response: Response,
        data: PredictionRequest,
        authorization: str = Header(None),
):
    &#34;&#34;&#34;
    The predict function is the main function of this API. It takes in a query and chat history,
    and returns a response from the model. The predict function also requires an authorization header
    which contains a JWT token that has been signed by our server.

    Args:
        data: PredictionRequest: Pass the query and chat history to the predict function
        authorization: str: Pass the jwt token to the function
        : Pass the query and chat history to the model

    Returns:
        A predictions response&#34;&#34;&#34;
    auth = decodeJWT(authorization)
    if auth[&#34;valid&#34;]:
        retriever = load_embedding(
            Param.EMBEDDING_SAVE_PATH + auth[&#34;data&#34;][&#34;username&#34;] + &#34;/&#34;
        )
        llms = retrieve_model(data, auth[&#34;data&#34;][&#34;username&#34;])
        chain = ConversationalRetrievalChain.from_llm(
            llm=llms, retriever=retriever.as_retriever()
        )
        result = LLM(chain, llms, retriever).predict(data.query, data.chat_history)

        return APIResponse(status=&#34;success&#34;, message=result)
    else:
        return HTTPException(401, detail=&#34;Unauthorised&#34;)</code></pre>
</details>
</dd>
<dt id="BTO_LLM_Backend.src.router.v1.endpoint.chatbot.retrieve_model"><code class="name flex">
<span>def <span class="ident">retrieve_model</span></span>(<span>data, username)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def retrieve_model(data, username):
    print(user_model_cache.keys())
    if data.use_default == 1:
        llms = llm
        return llms
    else:
        if (username in user_model_cache.keys() ):
            print(&#39;Exist Model in Cache&#39;)

            return user_model_cache[username][&#39;model&#39;]
        else:

            for i in user_model_cache.keys():
                if data.config.items() == user_model_cache[i][&#39;config&#39;].items():
                    print(&#39;Exist Model in Cache&#39;)
                    llms = user_model_cache[i][&#39;model&#39;]
                    return llms

            custom_llm = CTransformers(
                model=Param.LLM_MODEL_PATH,
                model_type=Param.LLM_MODEL_TYPE,
                config=data.config

            )
            user_model_cache[username] = {&#39;model&#39;: custom_llm, &#39;config&#39;: data.config}
            print(&#39;Created New Model in Cache&#39;)
            return user_model_cache[username][&#39;model&#39;]</code></pre>
</details>
</dd>
<dt id="BTO_LLM_Backend.src.router.v1.endpoint.chatbot.set_model"><code class="name flex">
<span>def <span class="ident">set_model</span></span>(<span>request: starlette.requests.Request, response: starlette.responses.Response, data: core.schema.prediction_request.ModelRequest, authorization: str = Header(None))</span>
</code></dt>
<dd>
<div class="desc"><p>The set_model function is a simple function that initialise the model for user.
&mdash;
description: Returns the health of this API.
responses:
200:
# HTTP Status code 200 means &quot;OK&quot; (the request was fulfilled)</p>
<h2 id="returns">Returns</h2>
<p>A dictionary with a key of &quot;status&quot; and a value of &quot;healthy&quot;</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@router.post(&#34;/set_model&#34;)
@limiter.limit(&#34;5/second&#34;)
def set_model(request: Request, response: Response, data: ModelRequest, authorization: str = Header(None),
              ):
    &#34;&#34;&#34;
    The set_model function is a simple function that initialise the model for user.
        ---
        description: Returns the health of this API.
        responses:
          200:  # HTTP Status code 200 means &amp;quot;OK&amp;quot; (the request was fulfilled)

    Returns:
        A dictionary with a key of &amp;quot;status&amp;quot; and a value of &amp;quot;healthy&amp;quot;
    &#34;&#34;&#34;
    auth = decodeJWT(authorization)
    if auth[&#34;valid&#34;]:
        try:
            if (data.config.items() == {
                &#34;max_new_tokens&#34;: Param.LLM_MAX_NEW_TOKENS,
                &#34;temperature&#34;: Param.LLM_TEMPERATURE,
                &#34;top_k&#34;: Param.TOP_K,
                &#34;top_p&#34;: Param.TOP_P,
                &#34;batch_size&#34;: Param.BATCH_SIZE

            }.items()):
                print(&#39;Default Config&#39;)
            else:
                exist = &#39;&#39;
                for i in user_model_cache.keys():
                    if data.config.items() == user_model_cache[i][&#39;config&#39;].items():
                        exist = i
                        print(&#39;Exist Model in Cache&#39;)

                if exist == &#39;&#39;:
                    custom_llm = CTransformers(
                        model=Param.LLM_MODEL_PATH,
                        model_type=Param.LLM_MODEL_TYPE,
                        config=data.config

                    )
                    user_model_cache[auth[&#34;data&#34;][&#34;username&#34;]] = {&#39;model&#39;: custom_llm, &#39;config&#39;: data.config}
                    print(&#39;Created New Model in Cache&#39;)

            return APIResponse(status=&#34;success&#34;, message=&#39;Model Initialisation Success&#39;)
        except Exception as e:
            print(e)
            return APIResponse(status=&#34;fail&#34;, message=&#39;Model Initialisation Failed&#39;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="BTO_LLM_Backend.src.router.v1.endpoint" href="index.html">BTO_LLM_Backend.src.router.v1.endpoint</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="BTO_LLM_Backend.src.router.v1.endpoint.chatbot.create_embedding" href="#BTO_LLM_Backend.src.router.v1.endpoint.chatbot.create_embedding">create_embedding</a></code></li>
<li><code><a title="BTO_LLM_Backend.src.router.v1.endpoint.chatbot.feedback" href="#BTO_LLM_Backend.src.router.v1.endpoint.chatbot.feedback">feedback</a></code></li>
<li><code><a title="BTO_LLM_Backend.src.router.v1.endpoint.chatbot.get_configuration" href="#BTO_LLM_Backend.src.router.v1.endpoint.chatbot.get_configuration">get_configuration</a></code></li>
<li><code><a title="BTO_LLM_Backend.src.router.v1.endpoint.chatbot.health_check" href="#BTO_LLM_Backend.src.router.v1.endpoint.chatbot.health_check">health_check</a></code></li>
<li><code><a title="BTO_LLM_Backend.src.router.v1.endpoint.chatbot.predict" href="#BTO_LLM_Backend.src.router.v1.endpoint.chatbot.predict">predict</a></code></li>
<li><code><a title="BTO_LLM_Backend.src.router.v1.endpoint.chatbot.retrieve_model" href="#BTO_LLM_Backend.src.router.v1.endpoint.chatbot.retrieve_model">retrieve_model</a></code></li>
<li><code><a title="BTO_LLM_Backend.src.router.v1.endpoint.chatbot.set_model" href="#BTO_LLM_Backend.src.router.v1.endpoint.chatbot.set_model">set_model</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>